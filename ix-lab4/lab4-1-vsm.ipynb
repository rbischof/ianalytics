{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** J\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Rafael Bischof\n",
    "* Jeniffer Lima Graf\n",
    "* Alexander Sanchez\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import scipy as sc\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lz = WordNetLemmatizer() \n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords |= {'cathedra', 'ex', 'course', 'exam', 'project', 'homework', 'student', 'professor', 'school', 'learn', 'learning', 'final', 'midterm', 'assessment', 'semester', 'prerequisite'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNgrams(l, N):\n",
    "    grams = []\n",
    "    for i in range(len(l)-1):\n",
    "        cword = l[i]\n",
    "        for n in range(N-1):\n",
    "            if i+n+1 < len(l):\n",
    "                cword += \" \" + l[i+n+1]\n",
    "                grams.append(cword)\n",
    "    return l + grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = {}\n",
    "wordcount = {}\n",
    "for course in courses:\n",
    "    l = [lz.lemmatize(w) for w in re.sub(r'[^\\w\\s]|[0-9]', '', course['description']).lower().split() if lz.lemmatize(w) not in stopwords]\n",
    "    \n",
    "    # add 2grams (more makes little sense)\n",
    "    l = createNgrams(l, 2)\n",
    "    \n",
    "    # keep track of number of occurrences of words\n",
    "    for w in l:\n",
    "        if w in wordcount:\n",
    "            wordcount[w] += 1\n",
    "        else:\n",
    "            wordcount.update({w:1})\n",
    "    \n",
    "    cs.update({course['courseId']: l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwords = set()\n",
    "for w in wordcount:\n",
    "    if (wordcount[w] > 10 and wordcount[w] < 1000) or w == 'facebook':\n",
    "        filterwords.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thow away most frequent and least frequent words\n",
    "for course in cs:\n",
    "    cs[course] = [w for w in cs[course] if w in filterwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseslist = list(cs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coursesIdx = {k: v for v, k in enumerate(courseslist)}\n",
    "idxCourses = {v: k for v, k in enumerate(courseslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordslist = list(filterwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsIdx = {k: v for v, k in enumerate(wordslist)}\n",
    "idxWords = {v: k for v, k in enumerate(wordslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 854)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF = np.zeros((len(wordslist), len(courseslist)))\n",
    "(len(wordslist), len(courseslist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF matrix\n",
    "for c in range(len(courseslist)):\n",
    "    occs = Counter(cs[courseslist[c]])\n",
    "    for w in range(len(wordslist)):\n",
    "        TF[w, c] = occs[wordslist[w]]\n",
    "\n",
    "TF /= np.max(TF, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 854)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = np.zeros((len(wordslist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IDF array\n",
    "prelog = np.log2(len(courseslist))\n",
    "for w in range(len(wordslist)):\n",
    "    occ = 0\n",
    "    for c in range(len(courseslist)):\n",
    "        if wordslist[w] in cs[courseslist[c]]:\n",
    "            occ += 1\n",
    "    IDF[w] = - np.log2(occ) + prelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TFIDF matrix\n",
    "TFIDF = TF * IDF.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_s = csr_matrix(TF, dtype=np.float64)\n",
    "IDF_s = csr_matrix(IDF.reshape(-1,1), dtype=np.float64)\n",
    "TFIDF_s = TF_s.multiply(IDF_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2333x854 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 77239 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc.sparse.save_npz('./TF.npz', TF_s)\n",
    "sc.sparse.save_npz('./TFIDF.npz', TFIDF_s)\n",
    "TF_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2333x854 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 77239 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicsForCourse(index, n=15):\n",
    "    indexes = np.argsort(TFIDF[:, index])[:-n-1:-1]\n",
    "    topics = []\n",
    "    for i in indexes:\n",
    "        topics.append(wordslist[i])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online',\n",
       " 'realworld',\n",
       " 'social',\n",
       " 'explore',\n",
       " 'mining',\n",
       " 'networking',\n",
       " 'largescale',\n",
       " 'service',\n",
       " 'internet',\n",
       " 'stream',\n",
       " 'data',\n",
       " 'ad',\n",
       " 'clustering',\n",
       " 'analytics',\n",
       " 'community']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 15 highest ranked words for course IX\n",
    "topicsForCourse(coursesIdx['COM-308'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topCoursesForWord(word, n):\n",
    "    indexes = np.argsort(TFIDF[wordsIdx[word]])[:-n-1:-1]\n",
    "    top = []\n",
    "    for i in indexes:\n",
    "        top.append(courseslist[i])\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(i, j):\n",
    "    di = TFIDF[:,i]\n",
    "    dj = TFIDF[:,j]\n",
    "    return np.dot(di, dj) / ( np.linalg.norm(di) * np.linalg.norm(dj) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaritiesCourses(courses):\n",
    "    indexes = []\n",
    "    for c in courses:\n",
    "        indexes.append(coursesIdx[c])\n",
    "        \n",
    "    m = np.zeros((5,5))\n",
    "    for i1 in range(len(indexes)):\n",
    "        for i2 in range(len(indexes)):\n",
    "            m[i1, i2] = round(sim(indexes[i1], indexes[i2]), 3)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for 'markov chain' ['COM-516', 'MGT-484', 'MATH-332', 'FIN-408', 'COM-308']\n",
      "Similarity between top courses\n",
      "[[1.    0.444 0.469 0.254 0.131]\n",
      " [0.444 1.    0.528 0.221 0.116]\n",
      " [0.469 0.528 1.    0.263 0.119]\n",
      " [0.254 0.221 0.263 1.    0.05 ]\n",
      " [0.131 0.116 0.119 0.05  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 courses for \\'markov chain\\'', topCoursesForWord('markov chain', 5))\n",
    "print('Similarity between top courses')\n",
    "print(similaritiesCourses(topCoursesForWord('markov chain', 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for 'facebook' ['EE-727', 'ENV-523', 'PHYS-615', 'MSE-656', 'CH-312']\n",
      "Similarity between top courses\n",
      "[[1.    0.005 0.005 0.045 0.01 ]\n",
      " [0.005 1.    0.009 0.04  0.039]\n",
      " [0.005 0.009 1.    0.058 0.012]\n",
      " [0.045 0.04  0.058 1.    0.022]\n",
      " [0.01  0.039 0.012 0.022 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 courses for \\'facebook\\'', topCoursesForWord('facebook', 5))\n",
    "print('Similarity between top courses')\n",
    "print(similaritiesCourses(topCoursesForWord('facebook', 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessedcourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(cs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('idxWords.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxWords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('idxCourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxCourses, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
