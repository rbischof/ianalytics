{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** J\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Rafael Bischof\n",
    "* Jeniffer Lima Graf\n",
    "* Alexander Sanchez\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import scipy as sc\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lz = WordNetLemmatizer() \n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords |= {'cathedra', 'ex', 'course', 'exam', 'project', 'homework', 'student', 'professor', 'school', 'learn', 'learning', 'final', 'midterm', 'assessment', 'semester', 'prerequisite'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rbischof/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createNgrams(l, N):\n",
    "    grams = []\n",
    "    for i in range(len(l)-1):\n",
    "        cword = l[i]\n",
    "        for n in range(N-1):\n",
    "            if i+n+1 < len(l):\n",
    "                cword += \" \" + l[i+n+1]\n",
    "                grams.append(cword)\n",
    "    return l + grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs = {}\n",
    "wordcount = {}\n",
    "for course in courses:\n",
    "    l = [lz.lemmatize(w) for w in re.sub(r'[^\\w\\s]|[0-9]', '', course['description']).lower().split() if lz.lemmatize(w) not in stopwords]\n",
    "    \n",
    "    # add 2grams (more makes little sense)\n",
    "    l = createNgrams(l, 2)\n",
    "    \n",
    "    # keep track of number of occurrences of words\n",
    "    for w in l:\n",
    "        if w in wordcount:\n",
    "            wordcount[w] += 1\n",
    "        else:\n",
    "            wordcount.update({w:1})\n",
    "    \n",
    "    cs.update({course['courseId']: l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterwords = set()\n",
    "for w in wordcount:\n",
    "    if (wordcount[w] > 2 and wordcount[w] < 1000):\n",
    "        filterwords.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thow away most frequent and least frequent words\n",
    "for course in cs:\n",
    "    cs[course] = [w for w in cs[course] if w in filterwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courseslist = list(cs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coursesIdx = {k: v for v, k in enumerate(courseslist)}\n",
    "idxCourses = {v: k for v, k in enumerate(courseslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordslist = list(filterwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsIdx = {k: v for v, k in enumerate(wordslist)}\n",
    "idxWords = {v: k for v, k in enumerate(wordslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate sparse TF matrix\n",
    "TFs = []\n",
    "rows = []\n",
    "cols = []\n",
    "for c in range(len(courseslist)):\n",
    "    occs = Counter(cs[courseslist[c]])\n",
    "    templist = []\n",
    "    mx = 0\n",
    "    for w in occs:\n",
    "        mx = max(mx, occs[w])\n",
    "        templist.append(occs[w])\n",
    "        rows.append(wordsIdx[w])\n",
    "        cols.append(c)\n",
    "    TFs += [x / mx for x in templist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDF = np.zeros((len(wordslist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate IDF array\n",
    "prelog = np.log2(len(courseslist))\n",
    "for w in range(len(wordslist)):\n",
    "    occ = 0\n",
    "    for c in range(len(courseslist)):\n",
    "        if wordslist[w] in cs[courseslist[c]]:\n",
    "            occ += 1\n",
    "    IDF[w] = - np.log2(occ) + prelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate sparse TFIDF matrix\n",
    "TFIDFs = [TFs[i] * IDF[rows[i]] for i in range(len(TFs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topicsForCourseSparse(index, n=15, M=TFIDFs, r=rows, c=cols):\n",
    "    top = []\n",
    "    toprows = []\n",
    "    for i in range(len(M)):\n",
    "        if index == c[i]:\n",
    "            top.append(M[i])\n",
    "            toprows.append(r[i])\n",
    "    indexes = np.argsort(top)[:-n-1:-1]\n",
    "    topics = []\n",
    "    for i in indexes:\n",
    "        topics.append((top[i], wordslist[toprows[i]]))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.8690461298102452, 'social networking'),\n",
       " (4.1089477813024056, 'online'),\n",
       " (4.1020865059328893, 'realworld'),\n",
       " (3.8406743688962703, 'social'),\n",
       " (3.7080820823665643, 'data mining'),\n",
       " (3.5638498912278198, 'explore'),\n",
       " (3.3690461298102452, 'mining'),\n",
       " (3.2080820823665643, 'networking'),\n",
       " (2.9126974198734965, 'hadoop'),\n",
       " (2.9126974198734965, 'community detection'),\n",
       " (2.8690461298102452, 'largescale'),\n",
       " (2.7177099196331111, 'recommender system'),\n",
       " (2.7177099196331111, 'recommender'),\n",
       " (2.7177099196331111, 'ecommerce'),\n",
       " (2.6072651517817387, 'service')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 15 highest ranked words for course IX\n",
    "topicsForCourseSparse(coursesIdx['COM-308'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest scores are obtained by terms that appear very prominently in the given course, but not so much in the rest of the corpus. These words therefore give us the most information about the course to distinguish it from others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topCoursesForWordSparse(word, n=15, M=TFIDFs, r=rows, c=cols):\n",
    "    top = []\n",
    "    topcols = []\n",
    "    for i in range(len(M)):\n",
    "        if wordsIdx[word] == r[i]:\n",
    "            top.append(M[i])\n",
    "            topcols.append(c[i])\n",
    "    indexes = np.argsort(top)[:-n-1:-1]\n",
    "    topcs = []\n",
    "    for i in indexes:\n",
    "        topcs.append(courses[topcols[i]]['name'])\n",
    "    return topcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We didn't manage to calculate the similarity between courses without using this library\n",
    "tf_idf = sc.sparse.csc_matrix((TFIDFs,(rows,cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simSparse(i, j, M=TFIDFs, r=rows, c=cols):\n",
    "    di = tf_idf.getcol(i).todense().reshape(-1)\n",
    "    dj = tf_idf.getcol(j).todense()\n",
    "    return np.dot(di, dj) / ( np.linalg.norm(di) * np.linalg.norm(dj) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similaritiesCourses(courses):\n",
    "    indexes = []\n",
    "    for c in courses:\n",
    "        indexes.append(coursesIdx[c])\n",
    "        \n",
    "    m = np.zeros((5,5))\n",
    "    for i1 in range(len(indexes)):\n",
    "        for i2 in range(len(indexes)):\n",
    "            m[i1, i2] = simSparse(indexes[i1], indexes[i2])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for 'markov chain' ['Markov chains and algorithmic applications', 'Applied probability & stochastic processes', 'Applied stochastic processes', 'Stochastic calculus I', 'Optimization and simulation']\n",
      "Similarity between top courses\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Markov chains and algorithmic applications'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-38dcba03df56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top 5 courses for \\'markov chain\\''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopCoursesForWordSparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'markov chain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Similarity between top courses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilaritiesCourses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopCoursesForWordSparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'markov chain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-6ad2344d02c5>\u001b[0m in \u001b[0;36msimilaritiesCourses\u001b[0;34m(courses)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoursesIdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Markov chains and algorithmic applications'"
     ]
    }
   ],
   "source": [
    "print('Top 5 courses for \\'markov chain\\'', topCoursesForWordSparse('markov chain', 5))\n",
    "print('Similarity between top courses')\n",
    "print(similaritiesCourses(topCoursesForWordSparse('markov chain', 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top course for 'facebook' ['Computational Social Media']\n",
      "It is actually the only course that contains the word facebook.\n"
     ]
    }
   ],
   "source": [
    "print('Top course for \\'facebook\\'', topCoursesForWordSparse('facebook', 5))\n",
    "print('It is actually the only course that contains the word facebook.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessedcourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(cs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/idxWords.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxWords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/idxCourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxCourses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/wordsIdx.pickle', 'wb') as handle:\n",
    "    pickle.dump(wordsIdx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/coursesIdx.pickle', 'wb') as handle:\n",
    "    pickle.dump(coursesIdx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "sc.sparse.save_npz(\"data/TFIDF.npz\", tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
