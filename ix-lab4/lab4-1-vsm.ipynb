{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** J\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Rafael Bischof\n",
    "* Jeniffer Lima Graf\n",
    "* Alexander Sanchez\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "ps = PorterStemmer()\n",
    "lz = WordNetLemmatizer() \n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords |= {'cathedra', 'ex', 'course', 'exam', 'project', 'homework', 'student', 'professor', 'school', 'learn', 'learning', 'final', 'midterm', 'assessment', 'semester', 'prerequisite'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNgrams(l, N):\n",
    "    grams = []\n",
    "    for i in range(len(l)-1):\n",
    "        cword = l[i]\n",
    "        for n in range(N-1):\n",
    "            if i+n+1 < len(l):\n",
    "                cword += \" \" + l[i+n+1]\n",
    "                grams.append(cword)\n",
    "    return l + grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = {}\n",
    "wordcount = {}\n",
    "for course in courses:\n",
    "    l = [lz.lemmatize(w) for w in re.sub(r'[^\\w\\s]|[0-9]', '', course['description']).lower().split() if lz.lemmatize(w) not in stopwords]\n",
    "    \n",
    "    # add 2grams (more makes little sense)\n",
    "    l = createNgrams(l, 2)\n",
    "    \n",
    "    # keep track of number of occurrences of words\n",
    "    for w in l:\n",
    "        if w in wordcount:\n",
    "            wordcount[w] += 1\n",
    "        else:\n",
    "            wordcount.update({w:1})\n",
    "    \n",
    "    cs.update({course['courseId']: l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwords = set()\n",
    "for w in wordcount:\n",
    "    if wordcount[w] > 10 and wordcount[w] < 1000:\n",
    "        filterwords.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thow away most frequent and least frequent words\n",
    "for course in cs:\n",
    "    cs[course] = [w for w in cs[course] if wordcount[w] < 1000 and wordcount[w] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseslist = list(cs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coursesIdx = {k: v for v, k in enumerate(courseslist)}\n",
    "idxCourses = {v: k for v, k in enumerate(courseslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordslist = list(filterwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsIdx = {k: v for v, k in enumerate(wordslist)}\n",
    "idxWords = {v: k for v, k in enumerate(wordslist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = np.zeros((len(wordslist), len(courseslist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF matrix\n",
    "for c in range(len(courseslist)):\n",
    "    occs = Counter(cs[courseslist[c]])\n",
    "    for w in range(len(wordslist)):\n",
    "        TF[w, c] = occs[wordslist[w]]\n",
    "\n",
    "TF /= np.max(TF, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = np.zeros((len(wordslist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IDF array\n",
    "prelog = np.log2(len(courseslist))\n",
    "for w in range(len(wordslist)):\n",
    "    occ = 0\n",
    "    for c in range(len(courseslist)):\n",
    "        if wordslist[w] in cs[courseslist[c]]:\n",
    "            occ += 1\n",
    "    IDF[w] = - np.log2(occ) + prelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TFIDF matrix\n",
    "TFIDF = TF * IDF.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicsForCourse(index, n=15):\n",
    "    indexes = np.argsort(TFIDF[:, index])[:-n-1:-1]\n",
    "    topics = []\n",
    "    for i in indexes:\n",
    "        topics.append(wordslist[i])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online',\n",
       " 'realworld',\n",
       " 'social',\n",
       " 'explore',\n",
       " 'mining',\n",
       " 'networking',\n",
       " 'largescale',\n",
       " 'service',\n",
       " 'internet',\n",
       " 'stream',\n",
       " 'data',\n",
       " 'ad',\n",
       " 'clustering',\n",
       " 'analytics',\n",
       " 'community']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 15 highest ranked words for course IX\n",
    "topicsForCourse(coursesIdx['COM-308'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topCoursesForWord(word, n):\n",
    "    indexes = np.argsort(TFIDF[wordsIdx[word]])[:-n-1:-1]\n",
    "    top = []\n",
    "    for i in indexes:\n",
    "        top.append(courseslist[i])\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(i, j):\n",
    "    di = TFIDF[:,i]\n",
    "    dj = TFIDF[:,j]\n",
    "    return np.dot(di, dj) / ( np.linalg.norm(di) * np.linalg.norm(dj) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaritiesCourses(courses):\n",
    "    indexes = []\n",
    "    for c in courses:\n",
    "        indexes.append(coursesIdx[c])\n",
    "        \n",
    "    m = np.zeros((5,5))\n",
    "    for i1 in range(len(indexes)):\n",
    "        for i2 in range(len(indexes)):\n",
    "            m[i1, i2] = round(sim(indexes[i1], indexes[i2]), 3)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 courses for 'markov chain' ['COM-516', 'MGT-484', 'MATH-332', 'FIN-408', 'COM-308']\n",
      "Similarity between top courses\n",
      "[[1.    0.444 0.469 0.254 0.131]\n",
      " [0.444 1.    0.528 0.221 0.116]\n",
      " [0.469 0.528 1.    0.263 0.119]\n",
      " [0.254 0.221 0.263 1.    0.05 ]\n",
      " [0.131 0.116 0.119 0.05  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 courses for \\'markov chain\\'', topCoursesForWord('markov chain', 5))\n",
    "print('Similarity between top courses')\n",
    "print(similaritiesCourses(topCoursesForWord('markov chain', 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'facebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-81e393a49f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top 5 courses for \\'facebook\\''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopCoursesForWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facebook'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Similarity between top courses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilaritiesCourses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopCoursesForWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facebook'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-31a747294f8b>\u001b[0m in \u001b[0;36mtopCoursesForWord\u001b[0;34m(word, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtopCoursesForWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordsIdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourseslist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'facebook'"
     ]
    }
   ],
   "source": [
    "print('Top 5 courses for \\'facebook\\'', topCoursesForWord('facebook', 5))\n",
    "print('Similarity between top courses')\n",
    "print(similaritiesCourses(topCoursesForWord('facebook', 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessedcourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(cs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('idxWords.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxWords, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('idxCourses.pickle', 'wb') as handle:\n",
    "    pickle.dump(idxCourses, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
